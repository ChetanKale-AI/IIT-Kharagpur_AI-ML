from transformers import AutoTokenizer, BertModel
import sys
import torch
import numpy as np
from torch.nn.functional import cosine_similarity  # To compute cosine similarity

if __name__ == "__main__":
    # Load the pre-trained BERT model
    model = BertModel.from_pretrained("bert-base-uncased")
    
    # Join all command-line arguments into a single string, excluding the script name (sys.argv[0])
    input_text = " ".join(sys.argv[1:])  # This captures the entire input as a string
    
    # Split the input based on the comma, assuming there's a space around the comma
    sentence1, sentence2 = input_text.split(" , ")

    sentences = [sentence1, sentence2]
    
    # Tokenize each sentence
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased", do_lower_case=True)
    sentences_tokenized = [tokenizer.tokenize(s) for s in sentences]
    MAX_LENGTH = max([len(sentences_tokenized[0]), len(sentences_tokenized[1])])

    # Generate token IDs for each sentence and pad them
    ids = [tokenizer.convert_tokens_to_ids(t) for t in sentences_tokenized]
    ids = np.asarray([np.pad(i, (0, MAX_LENGTH - len(i)), mode='constant') for i in ids])

    # Generate the attention masks
    amasks = np.asarray([[float(i > 0) for i in seq] for seq in ids])

    # Get the output from the BERT model
    output = model.forward(torch.tensor(ids), torch.tensor(amasks))

    # pool_vectors (2x768) is the vector generated by performing max-pooling over the hidden states
    pool_vectors = torch.max(output.last_hidden_state, dim=1).values  # Apply max-pooling over hidden states

    # cls_vectors (2x768) is the vector generated by taking the CLS token
    cls_vectors = output.pooler_output  # CLS token representation is stored in `pooler_output`

    # Compute the cosine similarity for the max-pooled vectors
    cosine_pooling = cosine_similarity(pool_vectors[0].unsqueeze(0), pool_vectors[1].unsqueeze(0))

    # Compute the cosine similarity for the CLS token vectors
    cosine_cls = cosine_similarity(cls_vectors[0].unsqueeze(0), cls_vectors[1].unsqueeze(0))

    # Finally print out the values rounded to 2 decimal places
    print(np.round(cosine_pooling.item(), 2), np.round(cosine_cls.item(), 2))
